# üß† Self-Correcting, Intent-Aware RAG  
### *A Failure-Aware Retrieval-Augmented Generation System (Free-Tier Only)*

<p align="center">
  <b>Adaptive Retrieval ‚Ä¢ Hallucination Detection ‚Ä¢ Self-Correction ‚Ä¢ Explicit Abstention</b>
</p>

---

## üö® Why This Project Exists (Problem Statement)

> **Modern RAG systems hallucinate not because they lack data ‚Äî  
> but because they lack epistemic awareness.**

Classic RAG pipelines assume:

\[
\text{High Similarity} \Rightarrow \text{High Truth}
\]

This assumption is **false**.

Semantic similarity does **not** guarantee:
- factual entailment
- temporal correctness
- contextual grounding

This project proposes a **verification-first RAG architecture** that treats hallucination as a *detectable failure mode*, not an accident.

---

## üß† Core Contributions (What Makes This ‚ÄúElite‚Äù)

<div style="border-left: 4px solid #6f42c1; padding-left: 12px;">
<b>Key Insight:</b><br>
Generation should be the <i>weakest</i> component in a RAG system ‚Äî  
verification and retrieval should dominate the pipeline.
</div>

This system introduces:

- **Intent-conditioned retrieval policies**
- **Formal grounding verification using NLI-style entailment**
- **Iterative, constraint-based self-correction**
- **Deterministic failure-aware abstention**

All implemented **without paid APIs or fine-tuning**.

---

## üë• Who Is This For?

This project is designed for:

- **Students** exploring advanced Retrieval-Augmented Generation (RAG) beyond basic LangChain demos  
- **Researchers** interested in hallucination detection, self-correcting pipelines, and verification-first LLM systems  
- **Engineers** who care more about *reliability and correctness* than raw fluency  
- **Evaluators / Professors** assessing system design, architectural clarity, and reasoning depth  

If you are looking for a chatbot demo ‚Äî this is not it.  
If you are interested in **building LLM systems that know when they might be wrong**, you‚Äôre in the right place.

---

## üß© System Overview (Formal Pipeline)

```mermaid
flowchart TD
    Q[User Query q] --> I[Intent Classification I(q)]
    I --> R[Adaptive Retrieval R_I(q)]
    R --> C[Retrieved Context C_k]
    C --> G[Initial RAG Generation]
    G --> V[Verification & NLI Entailment]

    V -->|Verified| A[Answer]
    V -->|Partially Grounded| S[Self-Correction]
    S --> V

    V -->|Low Confidence| T[Threshold Gate]
    T -->|Insufficient Evidence| X[Abstain]
```

---

## ‚öôÔ∏è How to Run
> **Prerequisites:** Python 3.10+, Git, and API keys for the listed free-tier LLM providers.

### 1. Clone the repository
```bash
git clone https://github.com/Raunak-23/Self_Correcting_RAG.git
cd <Self_Correcting_RAG>
```

### 2. Create and activate a virtual environment
```bash
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\\Scripts\\activate         # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up API keys
Create a .env file and add your API keys as per .env.example

### 5. Run the system
```bash
python main.py
```
This will execute the end-to-end pipeline:
intent classification ‚Üí adaptive retrieval ‚Üí generation ‚Üí verification ‚Üí correction / abstention.

## üß© System Overview (Formal Pipeline)

Let:
q       = user query
I(q)    = inferred intent
R_I(q)  = intent-specific retrieval policy
C_k     = retrieved context (top-k chunks)
G(.)    = generation function
V(.)    = verification function
 
The system executes:

q ‚Üí I(q) ‚Üí R_{I(q)} ‚Üí C_k ‚Üí G(q, C_k) ‚Üí V(claims, C_k)
        ‚Ü≥ Answer (if verified)
        ‚Ü≥ Correct (if salvageable)
        ‚Ü≥ Abstain (if unreliable)

---

## 1Ô∏è‚É£ Intent Classification ‚Äî *High-Dimensional Query Router*

I(q) = argmax_c P(c | q),  where c ‚àà C

**Why LLM-based classification?**  
It offers semantic generalization, structured outputs, and zero training cost.

**Model:** Llama 3.1 8B (Groq)

---

## 2Ô∏è‚É£ Adaptive Retrieval ‚Äî *Dynamic k-NN Controller*

k = f(I(q))   // number of retrieved chunks depends on intent

| Intent | Chunk Size | k | Strategy |
|------|-----------|---|----------|
| Factual | 256 | 5 | Precision-first |
| Comparative | 512 | 10 | Multi-query |
| Conceptual | 1024+ | 15 | Context-heavy |
| Ambiguous | variable | adaptive | HyDE |

### HyDE

√¢ = G(q, ‚àÖ)   // hypothetical answer generated without context

---

## 3Ô∏è‚É£ Initial RAG Generation

> Generation may be wrong ‚Äî verification may not.

**Model:** Gemini 3 Flash

---

## 4Ô∏è‚É£ Verification Judge ‚Äî *NLI Entailment Engine*

A ‚Üí { c1, c2, ..., cn }   // atomic claim decomposition

Each claim checked via entailment:

E(ci, Ck) ‚àà {entailed, neutral, contradicted}


**Model:** Nemotron-3 Nano 30B

---

## 5Ô∏è‚É£ Self-Correction ‚Äî *Constraint-Based Refinement*

A_{t+1} = argmin_Œî || Œî(A_t) ||

Preserves verified claims while removing hallucinations.

**Model:** Llama 4 Maverick

---

## 6Ô∏è‚É£ Failure-Aware Abstention ‚Äî *Threshold Gate*

Hard Abstention if:

max cosine_similarity < œÑ   (œÑ ‚âà 0.65)


Soft Abstention if <50% of claims are entailed.

<div style="border-left: 4px solid #d73a49; padding-left: 12px;">
<b>Abstention is a designed outcome, not a fallback.</b>
</div>

---

## üìä Evaluation

Evaluated using **RAGAS**:
- Faithfulness
- Context Precision
- Context Recall
- Answer Relevance

---

## üß≠ Design Philosophy

- Similarity ‚â† Truth  
- Retrieval > Generation  
- Verification > Fluency  
- Honesty > Confidence  

---

## üß† Final Remark

Traditional RAG systems are fluent.  
This one is careful.

That difference matters.
