# ğŸ§  Self-Correcting, Intent-Aware RAG  
### *A Failure-Aware Retrieval-Augmented Generation System (Free-Tier Only)*

<p align="center">
  <b>Adaptive Retrieval â€¢ Hallucination Detection â€¢ Self-Correction â€¢ Explicit Abstention</b>
</p>

---

## ğŸš¨ Why This Project Exists (Problem Statement)

> **Modern RAG systems hallucinate not because they lack data â€”  
> but because they lack epistemic awareness.**

Classic RAG pipelines assume:

\[
\text{High Similarity} \Rightarrow \text{High Truth}
\]

This assumption is **false**.

Semantic similarity does **not** guarantee:
- factual entailment
- temporal correctness
- contextual grounding

This project proposes a **verification-first RAG architecture** that treats hallucination as a *detectable failure mode*, not an accident.

---

## ğŸ§  Core Contributions (What Makes This â€œEliteâ€)

<div style="border-left: 4px solid #6f42c1; padding-left: 12px;">
<b>Key Insight:</b><br>
Generation should be the <i>weakest</i> component in a RAG system â€”  
verification and retrieval should dominate the pipeline.
</div>

This system introduces:

- **Intent-conditioned retrieval policies**
- **Formal grounding verification using NLI-style entailment**
- **Iterative, constraint-based self-correction**
- **Deterministic failure-aware abstention**

All implemented **without paid APIs or fine-tuning**.

---

## ğŸ‘¥ Who Is This For?

This project is designed for:

- **Students** exploring advanced Retrieval-Augmented Generation (RAG) beyond basic LangChain demos  
- **Researchers** interested in hallucination detection, self-correcting pipelines, and verification-first LLM systems  
- **Engineers** who care more about *reliability and correctness* than raw fluency  
- **Evaluators / Professors** assessing system design, architectural clarity, and reasoning depth  

If you are looking for a chatbot demo â€” this is not it.  
If you are interested in **building LLM systems that know when they might be wrong**, youâ€™re in the right place.

---

## ğŸ§© System Overview (Formal Pipeline)

```mermaid
flowchart TD
    Q[User Query q] --> I[Intent Classification]
    I --> R[Adaptive Retrieval]
    R --> C[Retrieved Context]
    C --> G[Initial RAG Generation]
    G --> V[Verification and Entailment]

    V -->|Verified| A[Answer]
    V -->|Partially Grounded| S[Self Correction]
    S --> V

    V -->|Low Confidence| T[Threshold Gate]
    T -->|Insufficient Evidence| X[Abstain]
```

---

## âš™ï¸ How to Run
> **Prerequisites:** Python 3.10+, Git, and API keys for the listed free-tier LLM providers.

### 1. Clone the repository
```bash
git clone https://github.com/Raunak-23/Self_Correcting_RAG.git
cd Self_Correcting_RAG
```

### 2. Create and activate a virtual environment
```bash
python -m venv venv
source venv/bin/activate        # Linux / macOS
venv\\Scripts\\activate         # Windows
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Set up API keys
Create a .env file and add your API keys as per .env.example :
```bash
QDRANT_URL=your_qdrant_url_here
QDRANT_API_KEY=your_qdrant_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here
GOOGLE_API_KEY=your_google_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

### 5. Run the system
```bash
python main.py
```
This will execute the end-to-end pipeline:
intent classification â†’ adaptive retrieval â†’ generation â†’ verification â†’ correction / abstention.

---

## ğŸ§© System Overview (Formal Pipeline)

Let:
q       = user query
I(q)    = inferred intent
R_I(q)  = intent-specific retrieval policy
C_k     = retrieved context (top-k chunks)
G(.)    = generation function
V(.)    = verification function
 
The system executes:

```text
q â†’ I(q) â†’ R_{I(q)} â†’ C_k â†’ G(q, C_k) â†’ V(claims, C_k)
        â†³ Answer (if verified)
        â†³ Correct (if salvageable)
        â†³ Abstain (if unreliable)
```

---

## 1ï¸âƒ£ Intent Classification â€” *High-Dimensional Query Router*

I(q) = argmax_c P(c | q),  where c âˆˆ C

**Why LLM-based classification?**  
It offers semantic generalization, structured outputs, and zero training cost.

**Model:** Llama 3.1 8B (Groq)

---

## 2ï¸âƒ£ Adaptive Retrieval â€” *Dynamic k-NN Controller*

k = f(I(q))   // number of retrieved chunks depends on intent

| Intent | Chunk Size | k | Strategy |
|------|-----------|---|----------|
| Factual | 256 | 5 | Precision-first |
| Comparative | 512 | 10 | Multi-query |
| Conceptual | 1024+ | 15 | Context-heavy |
| Ambiguous | variable | adaptive | HyDE |

### HyDE

Ã¢ = G(q, âˆ…)   // hypothetical answer generated without context

---

## 3ï¸âƒ£ Initial RAG Generation

> Generation may be wrong â€” verification may not.

**Model:** Gemini 3 Flash

---

## 4ï¸âƒ£ Verification Judge â€” *NLI Entailment Engine*

A â†’ { c1, c2, ..., cn }   // atomic claim decomposition

Each claim checked via entailment:

E(ci, Ck) âˆˆ {entailed, neutral, contradicted}


**Model:** Nemotron-3 Nano 30B

---

## 5ï¸âƒ£ Self-Correction â€” *Constraint-Based Refinement*

A_{t+1} = argmin_Î” || Î”(A_t) ||

Preserves verified claims while removing hallucinations.

**Model:** Llama 4 Maverick

---

## 6ï¸âƒ£ Failure-Aware Abstention â€” *Threshold Gate*

Hard Abstention if:

max cosine_similarity < Ï„   (Ï„ â‰ˆ 0.65)


Soft Abstention if <50% of claims are entailed.

<div style="border-left: 4px solid #d73a49; padding-left: 12px;">
<b>Abstention is a designed outcome, not a fallback.</b>
</div>

---

## ğŸ“Š Evaluation

Evaluated using **RAGAS**:
- Faithfulness
- Context Precision
- Context Recall
- Answer Relevance

---

## ğŸ§­ Design Philosophy

- Similarity â‰  Truth  
- Retrieval > Generation  
- Verification > Fluency  
- Honesty > Confidence  

---

## ğŸ§  Final Remark

Traditional RAG systems are fluent.  
This one is careful.

That difference matters.
